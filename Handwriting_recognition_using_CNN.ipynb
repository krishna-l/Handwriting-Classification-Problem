{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwriting recognition using CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q44mSFmvJANB",
        "colab_type": "text"
      },
      "source": [
        "# Handwriting Recognition using CNN\n",
        "Lets try to furthur optimize the solution by using Convolutional Neural Network which is better suited for image processing. CNNs are less sensitive to where in the image the pattern is that we are looking for. \n",
        "\n",
        "With MLP we achieved ~98% accuracy for test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwxEb_KJjWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3-JoVynKVkm",
        "colab_type": "text"
      },
      "source": [
        "Lets load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uopqbif-KX8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "145c0aee-97ec-44ee-b4ed-d4493c235271"
      },
      "source": [
        "(x_train, y_train),(x_test, y_test)=mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQnZUHY1X0vd",
        "colab_type": "text"
      },
      "source": [
        "We need to shape the data differently now. Since we have 2D images of 28x28 pixels, we need to set it up based on color channels in this case it will be 28x28x1 or 1x28x28 where 1 indicates color channes as its just grayscale. It would be 3 if we have RGB colors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV_pP9sWZbfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to check if the data is color channels first or in the end and then reshape the data\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  train_images = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
        "  test_images = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
        "  input_shape = (1, 28, 28) #shape of the input test data\n",
        "else:\n",
        "  train_images = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "  test_images = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "  input_shape = (28, 28, 1) #shape of the input test data\n",
        "\n",
        "#converting from 8bit byte data to float32\n",
        "train_images = train_images.astype('float32') \n",
        "test_images = test_images.astype('float32')\n",
        "train_images /= 255\n",
        "test_images /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haZ3brDqdaFm",
        "colab_type": "text"
      },
      "source": [
        "We need to convert our labels to one hot format ie [0,1,0,0,0,0,0,0,0,0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f99n_bAUdhMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
        "test_labels = tensorflow.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSlgHvvOe6SU",
        "colab_type": "text"
      },
      "source": [
        "Lets set up our CNN. \n",
        "We will start with 2D convolution of the image. Set up a 32 windows or filters of each image where each filter being 3x3 in kernel size.\n",
        "\n",
        "Another 2D convolution filter of 2x2 kernel size with relu activation is added with a 64 filters\n",
        "\n",
        "Lets apply MaxPooling2D layer that takes max of each 2x2 result to distill the results down to manageable size.\n",
        "\n",
        "Apply dropouts to manage overfitting. \n",
        "\n",
        "Next we flatten the 2D layer to a 1D layer at this point we do a tradition MLP and feed it to a 128 neuron MLP with relu activation\n",
        "\n",
        "Then we feed this data to out output neuron of 10 with softmax activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_x5XOAfggqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "eeb87a69-1cc6-4420-8ad3-d08919d3ca86"
      },
      "source": [
        "model = Sequential()\n",
        "#Adding the 32 2D convolution filter with kernel size 3x3 and relu acitvation function.\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "#Adding another 64 2D convolution filter with same config\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "#Reducing the samples to max of each 2x2 blocks\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#Adding dropout \n",
        "model.add(Dropout(0.25))\n",
        "#Flattening the result to 1D to feed it into a MLP\n",
        "model.add(Flatten())\n",
        "#Adding a hidden layer of 128 neurons with relu activation\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#adding another dropout\n",
        "model.add(Dropout(0.5))\n",
        "#output layer categoriztion with softmax activation\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mut-_oSyiFMr",
        "colab_type": "text"
      },
      "source": [
        "Lets check how our model has been programmed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WQmacb9iEck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "35be70b7-0c2e-49af-cc79-233547ecfe7a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp-dxp70jiqu",
        "colab_type": "text"
      },
      "source": [
        "Since we are dealing with multi category data we will do a categorical_crossentropy for the loss function and we will use RMSprop optimizer. (We can check with adam as well )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2XD8C9ljiT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UeXc0dHkJlE",
        "colab_type": "text"
      },
      "source": [
        "lets train our model. We will use a batch size of 32 and 10 epochs as this takes a lot of time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xshu9PcvmNlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f8016e8b-5cf2-49a5-c4fc-ac49988b4bfc"
      },
      "source": [
        "hist = model.fit(train_images, train_labels, batch_size=32, epochs=10, verbose=2, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 - 157s - loss: 0.1853 - acc: 0.9452 - val_loss: 0.0602 - val_acc: 0.9798\n",
            "Epoch 2/10\n",
            "60000/60000 - 158s - loss: 0.0875 - acc: 0.9741 - val_loss: 0.0527 - val_acc: 0.9826\n",
            "Epoch 3/10\n",
            "60000/60000 - 158s - loss: 0.0777 - acc: 0.9779 - val_loss: 0.0509 - val_acc: 0.9853\n",
            "Epoch 4/10\n",
            "60000/60000 - 158s - loss: 0.0783 - acc: 0.9782 - val_loss: 0.0591 - val_acc: 0.9823\n",
            "Epoch 5/10\n",
            "60000/60000 - 156s - loss: 0.0793 - acc: 0.9784 - val_loss: 0.0477 - val_acc: 0.9865\n",
            "Epoch 6/10\n",
            "60000/60000 - 156s - loss: 0.0845 - acc: 0.9766 - val_loss: 0.0551 - val_acc: 0.9827\n",
            "Epoch 7/10\n",
            "60000/60000 - 158s - loss: 0.0859 - acc: 0.9777 - val_loss: 0.0506 - val_acc: 0.9850\n",
            "Epoch 8/10\n",
            "60000/60000 - 157s - loss: 0.0913 - acc: 0.9757 - val_loss: 0.0567 - val_acc: 0.9824\n",
            "Epoch 9/10\n",
            "60000/60000 - 157s - loss: 0.0908 - acc: 0.9761 - val_loss: 0.1215 - val_acc: 0.9843\n",
            "Epoch 10/10\n",
            "60000/60000 - 157s - loss: 0.0960 - acc: 0.9754 - val_loss: 0.0693 - val_acc: 0.9807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqfHa6C4s7v0",
        "colab_type": "text"
      },
      "source": [
        "Lets check out test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_drHKn5s-tw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0e01cf26-8fb6-460a-bfc1-64f01ee1f2eb"
      },
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test Loss: ', score[0])\n",
        "print('Test Accuracy: ', score[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss:  0.06928741020411253\n",
            "Test Accuracy:  0.9807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwOuERfFtdDn",
        "colab_type": "text"
      },
      "source": [
        "Lets check by using adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rRQ5sjcthI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjQC6743t1gA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "c75043af-1ec0-4bc0-ecf8-afd432c873f0"
      },
      "source": [
        "hist = model.fit(train_images, train_labels, batch_size=32, epochs=10, verbose=2, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 - 152s - loss: 0.0880 - acc: 0.9762 - val_loss: 0.0428 - val_acc: 0.9882\n",
            "Epoch 2/10\n",
            "60000/60000 - 154s - loss: 0.0692 - acc: 0.9812 - val_loss: 0.0397 - val_acc: 0.9885\n",
            "Epoch 3/10\n",
            "60000/60000 - 154s - loss: 0.0608 - acc: 0.9828 - val_loss: 0.0394 - val_acc: 0.9888\n",
            "Epoch 4/10\n",
            "60000/60000 - 153s - loss: 0.0496 - acc: 0.9864 - val_loss: 0.0373 - val_acc: 0.9899\n",
            "Epoch 5/10\n",
            "60000/60000 - 155s - loss: 0.0457 - acc: 0.9869 - val_loss: 0.0382 - val_acc: 0.9901\n",
            "Epoch 6/10\n",
            "60000/60000 - 153s - loss: 0.0410 - acc: 0.9887 - val_loss: 0.0363 - val_acc: 0.9905\n",
            "Epoch 7/10\n",
            "60000/60000 - 154s - loss: 0.0366 - acc: 0.9894 - val_loss: 0.0359 - val_acc: 0.9906\n",
            "Epoch 8/10\n",
            "60000/60000 - 153s - loss: 0.0325 - acc: 0.9906 - val_loss: 0.0306 - val_acc: 0.9905\n",
            "Epoch 9/10\n",
            "60000/60000 - 153s - loss: 0.0329 - acc: 0.9902 - val_loss: 0.0319 - val_acc: 0.9897\n",
            "Epoch 10/10\n",
            "60000/60000 - 153s - loss: 0.0294 - acc: 0.9914 - val_loss: 0.0329 - val_acc: 0.9907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBQBIZBT1V-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "35757e18-4eac-4647-9007-b547c02cc06c"
      },
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test Loss: ', score[0])\n",
        "print('Test Accuracy: ', score[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss:  0.03287565022604867\n",
            "Test Accuracy:  0.9907\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}